{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4a7967",
   "metadata": {},
   "source": [
    "# Lab 3: LLM/Transformers and Language Representations\n",
    "\n",
    "This lab explores language model internals using Gemma-2-2b-it, covering text generation, attention mechanisms, hidden representations, and model interpretability through various exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce10e3e",
   "metadata": {},
   "source": [
    "### Model access and prerequisites\n",
    "\n",
    "This lab will use the off-the-shelf open-source LLM \"Gemma-2-2b-it\". This model is available on Hugging Face, which is a popular platform for sharing machine learning models. Please make an account there and go to \"https://huggingface.co/google/gemma-2-2b-it\" and accept the terms of use. Follow Instruction.md to set up your huggingface token as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6d867",
   "metadata": {},
   "source": [
    "## 1. Environment setup and model authentication\n",
    "\n",
    "Set up Hugging Face access and prepare the computing environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9911dcc",
   "metadata": {},
   "source": [
    "### Hugging Face authentication\n",
    "\n",
    "Load API key from environment and authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face API key from environment (do NOT hardcode your token here).\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file (if present)\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "if hf_key:\n",
    "    login(hf_key)\n",
    "else:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_API_KEY not found. Copy .env.template to .env and add your token. See Instruction.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263199a9",
   "metadata": {},
   "source": [
    "### PyTorch and CUDA configuration\n",
    "\n",
    "Configure PyTorch settings to avoid compilation issues and optimize CUDA performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PyTorch to avoid Windows CUDA compilation issues\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Disable torch compilation that can cause issues on Windows\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.config.disable = True\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "# Set safer CUDA configurations\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117e1f2",
   "metadata": {},
   "source": [
    "## 2. Model loading and helper functions\n",
    "\n",
    "Load the Gemma model, tokenizer, and set up utility functions for text generation and coherence scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3576fff",
   "metadata": {},
   "source": [
    "### Load Gemma model and tokenizer\n",
    "\n",
    "Let's load the model and tokenizer from Hugging Face and move the model to our GPU. We will use the `transformers` library for this purpose. Note that this may take a while, as the model is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a20521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"eager\",  # Force eager attention to enable output_attentions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d7fd9",
   "metadata": {},
   "source": [
    "### Coherence scoring model\n",
    "\n",
    "Next we download the ms-marco scoring model. This model is used to calculate the coherence score between the question and the answer. It is a small model, so it should load quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "SCORING_MODEL = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "SCORING_TOKENIZER = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "\n",
    "def calculate_coherence(question, answer, scoring_model=SCORING_MODEL, tokenizer=SCORING_TOKENIZER):\n",
    "  features = tokenizer([question], [answer], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  scoring_model.eval()\n",
    "  with torch.no_grad():\n",
    "      scores = scoring_model(**features).logits.squeeze().item()\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14223f",
   "metadata": {},
   "source": [
    "### Text generation helper function\n",
    "\n",
    "Define utility function for generating text from prompts with configurable sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c34597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_prompt(prompt, tokenizer, model, do_sample=False):\n",
    "  \"\"\"\n",
    "  generate the output from the prompt.\n",
    "  param:\n",
    "    prompt (str): the prompt inputted to the model\n",
    "    tokenizer   : the tokenizer that is used to encode / decode the input / output\n",
    "    model       : the model that is used to generate the output\n",
    "\n",
    "  return:\n",
    "    the response of the model\n",
    "  \"\"\"\n",
    "  # Tokenize the prompt\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  \n",
    "  # Generate response\n",
    "  with torch.no_grad():\n",
    "      output_ids = model.generate(\n",
    "          input_ids,\n",
    "          max_new_tokens=128,        # adjust as needed\n",
    "          do_sample=do_sample,           # deterministic\n",
    "          eos_token_id=tokenizer.eos_token_id,\n",
    "          pad_token_id=tokenizer.eos_token_id\n",
    "      )\n",
    "      \n",
    "\n",
    "  if output_ids is not None and len(output_ids) > 0:\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "  else:\n",
    "    return \"Empty Response\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf4be6",
   "metadata": {},
   "source": [
    "## 3. Text generation and coherence analysis\n",
    "\n",
    "Explore text generation with and without chat templates, and analyze response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335146eb",
   "metadata": {},
   "source": [
    "### Exercise 1: Text generation and coherence scoring\n",
    "\n",
    "Alright let's try the Gemma model with a simple prompt and calculate the coherence score of the response. A higher coherence score means that the response is more relevant to the question.\n",
    "\n",
    "**Exercise 1:** Generate text from the prompt and calculate the coherence score\n",
    "\n",
    "**Exercise 1b:** Switch do_sample to True and generate text again. What is the coherence score now? Why is it different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With chat template\n",
    "question = \"Please tell me about the key differences between supervised learning and unsupervised learning. Answer in 200 words.\"\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "prompt_with_template = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "response_with_template = generate_text_from_prompt(prompt_with_template, tokenizer, model, do_sample=False)\n",
    "\n",
    "# extract the real output from the model\n",
    "response_with_template = response_with_template.split('model\\n')[-1].strip('\\n').strip()\n",
    "\n",
    "print(\"========== Output ==========\\n\", response_with_template)\n",
    "score = calculate_coherence(question, response_with_template)\n",
    "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64543fa",
   "metadata": {},
   "source": [
    "### Comparison: With vs without chat template\n",
    "\n",
    "Compare text generation using chat templates versus plain text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without chat template (directly using plain text)\n",
    "response_without_template = generate_text_from_prompt(question, tokenizer, model)\n",
    "\n",
    "# extract the real output from the model\n",
    "response_without_template = response_without_template.split(question.split(' ')[-1])[-1].strip('\\n').strip()\n",
    "print(\"========== Output ==========\\n\", response_without_template)\n",
    "score = calculate_coherence(question, response_without_template)\n",
    "print(f\"========== Coherence Score : {score:.4f}  ==========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f601167",
   "metadata": {},
   "source": [
    "## 4. Output probability analysis and uncertainty\n",
    "\n",
    "Examine model output probabilities, entropy, and uncertainty through attention and logit analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d70bad",
   "metadata": {},
   "source": [
    "### Exercise 2: Output probabilities and entropy analysis\n",
    "\n",
    "Next we will inspect the attention layers and output logits/probabilities of the model. This will help us understand how the model processes the input and generates the output.\n",
    "\n",
    "**Exercise 2:** Fill in 3 consecutive user questions in the user_inputs list below. The model will generate a response and we plot the output probabilities for the top 10 tokens. \n",
    "\n",
    "**Exercise 2a:** Which prompt produced the lowest uncertainty? Measure this by calculating the truncated entropy of the 10 output logits. \n",
    "\n",
    "**Exercise 2b:** Now that we have some insight into the model's behaviour. Let's try to generate response with high uncertainty in the first token. With only changing the user inputs, can you generate a response with a low probability (< 0.5) for the first token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0061bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Glyph .* missing from font\"\n",
    ")                # suppress those emoji‐font warnings\n",
    "\n",
    "# ————— Student fills in their chat here ————— # TODO By student.\n",
    "user_inputs = [\n",
    "]\n",
    "# ——————————————————————————————————————————\n",
    "def entropy_topk(logits, T=1.0, k=40): ## TODO By student.\n",
    "    z = logits / T\n",
    "    topz, topi = torch.topk(z, k)\n",
    "    p = torch.softmax(topz, dim=-1)      # renormalized on top-k set\n",
    "    return float(-(p * (p.clamp_min(1e-12)).log()).sum())\n",
    "\n",
    "model = model.to(device)\n",
    "chat_history = []\n",
    "\n",
    "for turn, user_msg in enumerate(user_inputs, start=1):\n",
    "    # — 1) Append user message to history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    \n",
    "    # — 2) Show raw history + formatted prompt\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        chat_history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(f\"\\n--- Turn {turn} ---\")\n",
    "    print(\"Chat History:\")\n",
    "    print(json.dumps(chat_history, indent=2))\n",
    "    print(\"\\nFormatted Prompt:\")\n",
    "    print(formatted)\n",
    "    \n",
    "    # — 3) Tokenize & get logits → probs\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    last_logits = out.logits[:, -1, :]\n",
    "    probs = torch.nn.functional.softmax(last_logits, dim=-1)\n",
    "\n",
    "    # — 4) Top-10 tokens\n",
    "    top_k = 10\n",
    "    top_p, top_i = torch.topk(probs, top_k, dim=-1)\n",
    "    top_p = top_p.cpu().squeeze().numpy()\n",
    "    top_i = top_i.cpu().squeeze().numpy()\n",
    "    top_toks = [tokenizer.decode([i]) for i in top_i]\n",
    "    print(f\"Entropy of top-{top_k} tokens: {entropy_topk(last_logits)}\")\n",
    "    \n",
    "    # — 5) Plot immediately for this turn\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.barplot(x=top_p, y=top_toks, dodge=False)\n",
    "    plt.title(f\"Turn {turn}: Top-10 Next-Token Probs\")\n",
    "    plt.xlabel(\"Probability\")\n",
    "    plt.ylabel(\"Token\")\n",
    "    plt.legend([],[], frameon=False)   # hide redundant legend\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # — 6) Generate reply & append to history\n",
    "    gen = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature = 0.3\n",
    "    )\n",
    "    reply = tokenizer.decode(\n",
    "        gen[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    # — 7) Print assistant’s reply immediately\n",
    "    print(f\"\\nAssistant (Turn {turn}):\\n{reply}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c68bf",
   "metadata": {},
   "source": [
    "## 5. Sentence embeddings and layer analysis\n",
    "\n",
    "Explore sentence representations across different model layers and visualize semantic clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668fb15",
   "metadata": {},
   "source": [
    "### Exercise 3: Word embeddings and t-SNE visualization\n",
    "\n",
    "Model sentence/word representations are high-dimensional vectors that capture the meaning of the sentence/word. We can visualize these representations using t-SNE, which reduces the dimensionality of the vectors to 2D for visualization purposes.\n",
    "\n",
    "**Exercise 3:** Run the script below to visualize the sentence embeddings of the sentences. Why are the sentences clustered together like this?\n",
    "\n",
    "**Exercise 3a:** There is a conjecture that LLMs capture word-focused/lexical information in early layers, semantic information in middle layers, and sentence-level next token information in later layers. Find out how many layers the model has and print it out.\n",
    "\n",
    "**Exercise 3b:** Compare the t-SNE cluster of early, middle, and late layers. Do you see any differences in the clustering of the sentences? Provide explanations based on the above conjecture.\n",
    "\n",
    "**Exercise 3c:** Change the sentence for Orange (telecom) to align better with Microsoft (company) and Apple (company) based on your insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b08ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Sentences with different meanings of words\n",
    "sentences = [\n",
    "    \"I ate a fresh apple.\",  # Apple (fruit)\n",
    "    \"Apple released the new iPhone.\",  # Apple (company)\n",
    "    \"I peeled an orange and ate it.\",  # Orange (fruit)\n",
    "    \"The Orange network has great coverage.\",  # Orange (telecom)\n",
    "    \"Microsoft announced a new update.\",  # Microsoft (company)\n",
    "    \"Banana is my favorite fruit.\",  # Banana (fruit)\n",
    "    \"My company eats fruits for lunch.\",  # Company (business)\n",
    "]\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# Compute sentence-level embeddings (mean pooling with attention mask to ignore padding)\n",
    "hidden_states = outputs.hidden_states[-1]            # [B, T, D]\n",
    "mask = inputs.attention_mask.unsqueeze(-1)           # [B, T, 1]\n",
    "sum_vec = (hidden_states * mask).sum(dim=1)          # [B, D]\n",
    "len_vec = mask.sum(dim=1).clamp_min(1)               # [B, 1]\n",
    "sentence_embeddings = (sum_vec / len_vec).cpu().numpy()\n",
    "\n",
    "# Words to visualize\n",
    "word_labels = [\n",
    "    \"Apple (fruit)\", \"Apple (company)\",\n",
    "    \"Orange (fruit)\", \"Orange (telecom)\",\n",
    "    \"Microsoft (company)\", \"Banana (fruit)\",\n",
    "]\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [\"red\", \"blue\", \"orange\", \"purple\", \"green\", \"brown\", \"pink\", \"cyan\", \"magenta\", \"yellow\"]\n",
    "for i, label in enumerate(word_labels):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=colors[i], s=100)\n",
    "    plt.text(embeddings_2d[i, 0] + 0.1, embeddings_2d[i, 1] + 0.1, label, fontsize=12, color=colors[i])\n",
    "\n",
    "plt.xlabel(\"t-SNE Dim 1\")\n",
    "plt.ylabel(\"t-SNE Dim 2\")\n",
    "plt.title(\"t-SNE Visualization of Word Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72284b1",
   "metadata": {},
   "source": [
    "## 6. Attention mechanism analysis\n",
    "\n",
    "Inspect and interpret attention patterns during text generation across different layers and heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9fb7df",
   "metadata": {},
   "source": [
    "### Exercise 4: Attention pattern inspection during generation\n",
    "\n",
    "Inspecting and interpreting attention during generation.\n",
    " \n",
    "Have a look at the code and try to understand what the different components do.\n",
    "\n",
    "**Exercise 4:** Fill in the code to extract the logits and attentions each step.\n",
    "\n",
    "**Exercise 4a:** Plot the attention heatmap for the first layer and head 7. Pick two noteworthy query tokens and explain the attention patterns for each.\n",
    "\n",
    "**Exercise 4b:** Try out a different layer/head and explain one attention pattern you observe. How does it differ from the previous layer/head?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Input prompt for text generation\n",
    "prompt = \"Google\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")  # Tokenize the input prompt\n",
    "next_token_id = input_ids.input_ids.to(\"cuda\")  # Move input token ids to GPU\n",
    "attention_mask = input_ids.attention_mask.to(\"cuda\")  # Move attention mask to GPU\n",
    "cache_position = torch.arange(attention_mask.shape[1], device=\"cuda\")  # Position for the KV cache\n",
    "\n",
    "# Set the number of tokens to generate and other parameters\n",
    "generation_tokens = 20  # Limit for visualization (number of tokens to generate)\n",
    "total_tokens = generation_tokens + next_token_id.size(1) - 1  # Total tokens to handle\n",
    "layer_idx = 10  # Specify the layer index for attention visualization\n",
    "head_idx = 7  # Specify the attention head index to visualize\n",
    "\n",
    "# KV cache setup for caching key/values across time steps\n",
    "from transformers.cache_utils import HybridCache\n",
    "kv_cache = HybridCache(config=model.config, max_batch_size=1, max_cache_len=total_tokens, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "generated_tokens = []  # List to store generated tokens\n",
    "attentions = None  # Placeholder to store attention weights\n",
    "\n",
    "num_new_tokens = 0  # Counter for the number of new tokens generated\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# Generate tokens and collect attention weights for visualization\n",
    "for num_new_tokens in range(generation_tokens):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            next_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            use_cache=True,\n",
    "            past_key_values=kv_cache,\n",
    "            output_attentions=True\n",
    "        )\n",
    "\n",
    "    ## TODO Student\n",
    "    # Get the logits for the last generated token from outputs\n",
    "    logits = \n",
    "    \n",
    "    ## TODO Student\n",
    "    # Extract the attention scores from the model's outputs\n",
    "    attention_scores =\n",
    "    \n",
    "    # Extract attention weights for the specified layer and head\n",
    "    last_layer_attention = attention_scores[layer_idx][0][head_idx].detach().cpu().numpy()\n",
    "\n",
    "    if num_new_tokens == 0:\n",
    "        attentions = last_layer_attention\n",
    "    else:\n",
    "        attentions = np.append(attentions, last_layer_attention, axis=0)\n",
    "\n",
    "    # Greedy next‐token selection (you could swap in sampling if you like)\n",
    "    next_token_id = logits.argmax(dim=-1)           # shape: [1]\n",
    "    generated_tokens.append(next_token_id.item())\n",
    "\n",
    "    # Update masks, cache, and positions for the next step\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones(1, 1, device=\"cuda\")], dim=-1)\n",
    "    next_token_id = next_token_id.unsqueeze(0)\n",
    "    kv_cache = outputs.past_key_values\n",
    "    cache_position = cache_position[-1:] + 1\n",
    "    \n",
    "\n",
    "# Decode the generated tokens into human-readable text\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "full_text = prompt + generated_text  # Combine the prompt with the generated text\n",
    "\n",
    "\n",
    "# Tokenize all the generated text (prompt + generated)\n",
    "tokens = tokenizer.tokenize(full_text)\n",
    "\n",
    "# Function to plot a heatmap of attention weights\n",
    "def plot_attention(attn_matrix, tokens, title=\"Attention Heatmap\"):\n",
    "    plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "    sns.heatmap(attn_matrix, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=False)  # Plot the attention matrix as a heatmap\n",
    "    plt.xlabel(\"Key Tokens\")\n",
    "    plt.ylabel(\"Query Tokens\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "    plt.yticks(rotation=0)  # Rotate y-axis labels\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Generated Text: {full_text}\")\n",
    "# Plot the attention heatmap for the last generated token\n",
    "plot_attention(attentions, tokens, title=f\"Attention Weights for Generated Token of Layer {layer_idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
