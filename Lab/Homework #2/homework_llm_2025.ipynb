{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14129bc1",
   "metadata": {},
   "source": [
    "# Homework - Fine-tuning leads to forgetting\n",
    "In this homework you will use your acquired knowledge of fine-tuning large language models (LLMs) to improve the performance of a model on the GSM8K mathematics dataset. You will implement a few-shot learning approach and evaluate the model's performance on the public test set BoolQ.\n",
    "Because this homework may be challenging, a lot of code is already provided, and you have a clear task of improving the performance of the model in terms of accuracy on the fine-tuning task and absence of forgetting on the evaluation task.\n",
    "\n",
    "## Instructions\n",
    "Submit your assignment as Jupter notebook with all relevant execution outputs visible. Please clearly indicate and motivate the relevant steps in your code using comments. Partial grades can be given for incomplete results when the steps are clearly marked. \n",
    "\n",
    "Be sure to use comments throughout your code to clearly indicate the purpose of each section and the specific steps you are performing. While complete results are preferred, partial grades may be given if the steps are clearly marked and the work is well-documented.\n",
    "\n",
    "Report your scores on the GMS8K test set and the BoolQ test set in the notebook. \n",
    "Explanations on why your parameters/tactics are chosen for each task are required and if the score improvement is available, you can report it as well.\n",
    "\n",
    "Task list (you may complete the tasks in any order):\n",
    "\n",
    "1. Set up the LoRA configuration. (2 point)\n",
    "2. Set up fixed few-shot example selection. (You may even use your own few-shot examples, but make sure to document them in the notebook) (3 points)\n",
    "3. Set the number of few-shot examples during training and evaluation. (1 point)\n",
    "4. Add weight decay to the optimizer. (1 point)\n",
    "5. Add a learning rate scheduler. (1 point)\n",
    "6. Add a reasonable warmup ratio. (1 point)\n",
    "7. Decrease the learning rate. (1 point)\n",
    "8. Alter the number of training epochs if necessary. (1 point)\n",
    "\n",
    "### After training\n",
    "\n",
    "9. Set up greedy decoding for inference. (1 point)\n",
    "10. Set the inference max_new_tokens parameter to a suitable value. (2 points)\n",
    "11. Set checkpoint for loading the inference model. (1 point)\n",
    "12. Set the test number of few-shot examples. (1 point)\n",
    "13. For full evaluation, increase the ``EVAL_LIMIT`` to ``None``. (1 point)\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- A. Fine-tune the model on the GSM8K dataset using the LoRA configuration.\n",
    "- B. Evaluate the model on the GSM8K test set and tweak your setup to get the accuracy above 50%. (5 points if above threshold)\n",
    "- C. Evaluate the model on the 100 samples of the BoolQ test set and tweak your setup to avoid forgetting. Thus make sure the accuracy does not significantly decrease from the baseline accuracy of 66% on the 100 samples of the BoolQ test set while achieving good performance on the GSM8K test set. (5 points if above threshold)\n",
    "- D. Report the final scores on both test sets in the notebook and explain your choices for each task in a markdown cell. **Make sure to properly format your explanation and scores in the answer cell below. Unclear answers will be assigned 0 points.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23b078",
   "metadata": {},
   "source": [
    "ANSWER CELL\n",
    "\n",
    "**Student name:**  \n",
    "**Student email:**  \n",
    "**Date:**  \n",
    "\n",
    "---\n",
    "\n",
    "**Task 1:**\n",
    "- Choice:\n",
    "- Motivation:\n",
    "\n",
    "**Task 2:**\n",
    "- Choice:\n",
    "- Motivation:\n",
    "\n",
    "**Task 3:**\n",
    "- Choice:\n",
    "- Motivation:\n",
    "\n",
    "**Task i:**\n",
    "- Choice:\n",
    "- Motivation:\n",
    "\n",
    "Use the above task format for all tasks.\n",
    "\n",
    "> **Submission checklist (student):**  \n",
    "> - [ ] All “Choice” fields filled with the chosen approach\n",
    "> - [ ] All “Motivation” fields (2–4 sentences)  \n",
    "> - [ ] All code is reproducible\n",
    "\n",
    "**Objective B:**\n",
    "- Result:\n",
    "- Explanation: \n",
    "\n",
    "**Objective C:**\n",
    "- Result:\n",
    "- Explanation: \n",
    "\n",
    "> **Submission checklist (student):**  \n",
    "> - [ ] All “Result\" fields filled with the obtained results\n",
    "> - [ ] All ”Explanation\" fields (5–10 sentences) filled with an analysis of the results.\n",
    "> - [ ] All code is reproducible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a7336",
   "metadata": {},
   "source": [
    "## 1. Environment and data setup\n",
    "\n",
    "Set up GPU availability, download datasets, and authenticate with Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dce4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure we have a GPU available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cfcf4",
   "metadata": {},
   "source": [
    "### Download datasets\n",
    "\n",
    "The code block below downloads the relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "# Create an SSL context that doesn't verify certificates\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "urls = [\n",
    "    ('https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl', 'gsm8k_train.jsonl'),\n",
    "    ('https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl', 'gsm8k_test_public.jsonl'),\n",
    "]\n",
    "\n",
    "for url, filename in urls:\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    try:\n",
    "        # Create a request with custom SSL context\n",
    "        request = urllib.request.Request(url)\n",
    "        with urllib.request.urlopen(request, context=ssl_context) as response:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.read())\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702be80",
   "metadata": {},
   "source": [
    "### Authenticate to Hugging Face and get model access\n",
    "\n",
    "Login to huggingface and make sure you have gained access to the LLama-3.2-1B-Instruct model. Acces can be gained at [huggingface](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) by clicking on the \"Request Access\" button. After you have access, you can use the code below to download the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face API key from environment (do NOT hardcode your token here).\n",
    "# Load Hugging Face API key from environment (do NOT hardcode your token here).\n",
    "import os\n",
    "import logging, warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Silence transformers/TRL logs early\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"trl\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hide specific noisy warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*loss_type=None.*ForCausalLMLoss.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*cuDNN SDPA backward got grad_output\\.strides\\(\\) != output\\.strides\\(\\).*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"  \n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file (if present)\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "if hf_key:\n",
    "    login(hf_key)\n",
    "else:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_API_KEY not found. Copy .env.template to .env and add your token. See Instruction.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a01598",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Below are the necessary imports for the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adacdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
    "    AutoTokenizer, # imports the tokenizer for the model\n",
    "    pipeline # imports the pipeline for text generation\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, # imports the configuration for LoRA\n",
    "    get_peft_model, # imports the function to get the PEFT model\n",
    "    PeftModel # imports the PEFT model\n",
    ")\n",
    "import json\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Sets the CUDA device to use\n",
    "device = torch.device('cuda:0') # Creates a CUDA device object\n",
    "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
    "from trl import SFTConfig, SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
    "import random\n",
    "random.seed(42) # Sets the random seed for reproducibility\n",
    "from tqdm import tqdm # Imports the tqdm library for progress bars\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa7e04",
   "metadata": {},
   "source": [
    "## 2. Model, LoRA configuration, and data preparation\n",
    "\n",
    "Import libraries, configure the base model and LoRA, and prepare few-shot data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3f2b3",
   "metadata": {},
   "source": [
    "### Model and LoRA configuration\n",
    "\n",
    "Import the model and setup your LoRA configuration in a reasonable way.\n",
    "\n",
    "**Task 1: Set up the LoRA configuration. (2 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de61df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model_name = 'meta-llama/Llama-3.2-1B-Instruct'  # No quantization\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    "    torch_dtype=torch.float16,  # No quantization\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "base_model = copy.deepcopy(sft_model)  # Keep a copy of the base model for evaluation\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    ")\n",
    "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "peft_config = LoraConfig( ## TODO: Add LoRA parameters\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "peft_model = get_peft_model(sft_model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf0f4c",
   "metadata": {},
   "source": [
    "### Data helpers and few-shot strategy\n",
    "\n",
    "Below are helpers to read the train set and setup the few-shot learning approach. The current approach uses random sampling to select the few-shot examples. Change this to a fixed and sophisticated approach.\n",
    "\n",
    "**Task 2. Set up fixed few-shot example selection. (3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee13933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonlines(file_name: str):\n",
    "    f = open(file_name, 'r')\n",
    "    return [json.loads(line) for line in f]\n",
    "\n",
    "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
    "    if mode not in ['train', 'test']:\n",
    "        raise AssertionError('Undefined Mode!!!')\n",
    "\n",
    "    chats = []\n",
    "    # TODO: Use fixed few-shot examples\n",
    "    for qna in random.sample(nshot_data, n): # Samples n examples from the n-shot data\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
    "            }\n",
    "        )\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    chats.append(\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
    "        }\n",
    "    )\n",
    "    if mode == 'train':\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chats # Returns the list of chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d07f06",
   "metadata": {},
   "source": [
    "### Prepare training data and choose N-shot examples\n",
    "\n",
    "You may choose your number of few-shot examples yourselves to maximize performance and minimize unnecessary compute. When playing around, limit the number of examples in the dataset to a small number to speed up the process.\n",
    "\n",
    "**Task 3: Set the number of few-shot examples during training and evaluation. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_train = load_jsonlines('gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "\n",
    "formatted_gsm8k = []\n",
    "TRAIN_N_SHOT = 0 # TODO: Give model more examples\n",
    "TRAIN_LIMIT = None # Limit the number of examples for testing\n",
    "max_token_len = 0 # Record token length of dataset and prevent data from truncation\n",
    "for i, qna in enumerate(gsm8k_train): # Iterates over the GSM8K training data\n",
    "    chats = nshot_chats(nshot_data=gsm8k_train, n=TRAIN_N_SHOT, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
    "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
    "\n",
    "    train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):] # Remove Cutting Knowledge Date in prompt template\n",
    "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
    "        {\n",
    "            'text': train_sample # Adds the text of the example\n",
    "        }\n",
    "    )\n",
    "    max_token_len = max(max_token_len, len(sft_tokenizer(train_sample)['input_ids'])) # Updates the maximum token length\n",
    "    if TRAIN_LIMIT and i > TRAIN_LIMIT: # Limit the number of examples for testing\n",
    "        break\n",
    "    \n",
    "print(f\"Last example: {train_sample}\")\n",
    "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152dbc6e",
   "metadata": {},
   "source": [
    "## 3. Training (SFT + LoRA)\n",
    "\n",
    "Configure and run supervised fine-tuning with LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38625b0f",
   "metadata": {},
   "source": [
    "### Configure training\n",
    "\n",
    "The code block below sets up the training. Configure reasonable parameters.\n",
    "\n",
    "\n",
    "**Task 4: Add weight decay to the optimizer. (1 point)**\n",
    "\n",
    "**Task 5: Add a learning rate scheduler. (1 point)**\n",
    "\n",
    "**Task 6: Add a reasonable warmup ratio. (1 point)**\n",
    "\n",
    "**Task 7: Decrease the learning rate. (1 point)**\n",
    "\n",
    "**Task 8: Alter the number of training epochs if necessary. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f414d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
    "    seed=1126,\n",
    "    data_seed=1126,\n",
    "    output_dir=f\"sft\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1, # TODO: If you use fixed few-shot examples, possibly increase epochs\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    learning_rate=2e-4, # TODO: Decrease learning rate  # TODO: Add weight decay # TODO: Add warmup ratio # TODO: Add lr scheduler \n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    max_seq_length=max_token_len,\n",
    "    dataset_text_field='text',\n",
    "    report_to='none',\n",
    ")\n",
    "print(f\"On device: {device}\")\n",
    "trainer = SFTTrainer( # Creates the SFT trainer\n",
    "    model=peft_model,\n",
    "    train_dataset=formatted_gsm8k,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=sft_tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "trainer.train() # Starts the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f5f8e",
   "metadata": {},
   "source": [
    "## 4. Inference and checkpoints\n",
    "\n",
    "Load checkpoints, set greedy decoding, and define helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076825e5",
   "metadata": {},
   "source": [
    "### Load checkpoint and set up inference\n",
    "\n",
    "Now load a checkpoint of your choice and set up inference. Switch the inference to greedy decoding instead of sampling.\n",
    "\n",
    "**Task 9: Set up greedy decoding for inference. (1 point)**\n",
    "\n",
    "**Task 10: Set the inference max_new_tokens parameter to a suitable value. (2 points)**\n",
    "\n",
    "**Task 11: Set checkpoint for loading the inference model. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eedf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline( # Creates a text generation pipeline\n",
    "    'text-generation',\n",
    "    model=sft_model,\n",
    "    tokenizer=sft_tokenizer,\n",
    "    pad_token_id=sft_tokenizer.eos_token_id,\n",
    "    max_new_tokens=64, # TODO: Increase max_new_tokens for longer output\n",
    "    # TODO: Use greedy decoding strategy\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "adapter_path = 'sft/checkpoint-1122' # TODO: Evaluate different checkpoints\n",
    "pipeline.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
    "    sft_model,\n",
    "    adapter_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80653c17",
   "metadata": {},
   "source": [
    "### Inference helpers\n",
    "\n",
    "Utility functions for generation and answer extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response(chats: list): # Function to get the response from the model\n",
    "    # Apply chat template just like in training data preparation\n",
    "    formatted_prompt = sft_tokenizer.apply_chat_template(chats, tokenize=False, add_generation_prompt=True)\n",
    "    # Remove system prompt part to match training format\n",
    "    if \"<|eot_id|>\" in formatted_prompt:\n",
    "        formatted_prompt = formatted_prompt[formatted_prompt.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):]\n",
    "\n",
    "    gen_text = generator(formatted_prompt)[0]  # Generate from formatted prompt\n",
    "    \n",
    "    return gen_text['generated_text'][len(formatted_prompt):].strip() # Return only the new generated text\n",
    "\n",
    "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
    "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
    "\n",
    "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    return answer # Returns the extracted answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e1184",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Evaluate on GSM8K (fine-tuned task) and BoolQ (forgetting check)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f4906",
   "metadata": {},
   "source": [
    "### Evaluate on GSM8K\n",
    "\n",
    "Great! You have now set up the training and inference. The next step is to evaluate the model on the fine-tuning task. Configure your number of shots and limit the number of examples when playing around to speed up the process. You must report the final accuracy on the fine-tuning task on the whole set when you submit your homework.\n",
    "\n",
    "**Task 12: Set the test number of few-shot examples. (1 point)**\n",
    "\n",
    "**Task 13: For full evaluation, increase the ``EVAL_LIMIT`` to ``None``. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_predictions = []\n",
    "TEST_N_SHOT = 0 # TODO: give model more examples\n",
    "EVAL_LIMIT = 20 # TODO: Change to None for full evaluation or keep it for testing\n",
    "gsm8k_test_public = load_jsonlines('gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
    "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
    "\n",
    "correct = 0\n",
    "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
    "    if EVAL_LIMIT and i >= EVAL_LIMIT - 1:\n",
    "        break\n",
    "    \n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
    "    if i < 3:\n",
    "        print(f\"Example {i+1}/{gsm8k_total}: Question: {qna['question']}\") # Prints the question for the current example\n",
    "        print(f\"Example {i+1}/{gsm8k_total}: Response: {response}\") # Prints the response for the current example\n",
    "        print(f\"Example {i+1}/{gsm8k_total}: Predicted Answer: {pred_ans}, True Answer: {true_ans}\") # Prints the predicted and true answers for the current example\n",
    "        print(\"\")\n",
    "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
    "        correct += 1 # Increments the correct count if the prediction is correct\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "    \n",
    "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "\n",
    "print(f\"Predicted last answer: {pred_ans}\") # Prints the last predicted answer\n",
    "print(f\"True last answer: {true_ans}\") # Prints the true answer of the last example\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/EVAL_LIMIT:.3f}') # Prints the total accuracy on the public test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad003ec1",
   "metadata": {},
   "source": [
    "### Evaluate on BoolQ\n",
    "\n",
    "Now we evaluate how much the model has forgotten on an evaluation task. The evaluation task is the BoolQ dataset, which is a binary question answering dataset. The model should not have been trained on this dataset. The model should only be able to answer the questions based on its general knowledge. The submitted homework should report the accuracy on the 100 examples in the BoolQ dataset below. The base model score is 66%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "boolq = load_dataset(\"boolq\")\n",
    "boolq_df = pd.DataFrame(boolq['validation'])\n",
    "\n",
    "sample_size = min(100, len(boolq_df))\n",
    "df_sample = boolq_df.sample(n=sample_size, random_state=45)\n",
    "\n",
    "correct = 0\n",
    "results = []\n",
    "\n",
    "boolq_progress_bar = tqdm(total=sample_size, desc='BoolQ Evaluation')\n",
    "\n",
    "for i, (idx, row) in enumerate(df_sample.iterrows()):\n",
    "    question = row['question']\n",
    "    correct_answer = row['answer']  # True/False\n",
    "    \n",
    "    # Create chat format just like in training\n",
    "    messages = [{'role': 'user', 'content': f\"Question: {question}\\n\\nAnswer with just 'Yes' or 'No':\"}]\n",
    "    response = get_response(messages)  # This will now apply chat template consistently\n",
    "    \n",
    "    # Extract Yes/No from response\n",
    "    response_lower = response.lower()\n",
    "    if 'yes' in response_lower[:30]:  # Look in first 30 chars\n",
    "        predicted = True\n",
    "    elif 'no' in response_lower[:30]:\n",
    "        predicted = False\n",
    "    else:\n",
    "        predicted = None\n",
    "        \n",
    "    is_correct = predicted == correct_answer\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "        \n",
    "    results.append({\n",
    "        'question': question,\n",
    "        'correct_answer': correct_answer,\n",
    "        'predicted': predicted,\n",
    "        'is_correct': is_correct,\n",
    "        'response': response\n",
    "    })\n",
    "    if i < 3:\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Q: {question}\")\n",
    "        print(f\"  Correct: {correct_answer}, Predicted: {predicted}, Right: {is_correct}\")\n",
    "        print(f\"  Response: {response}\")\n",
    "        print()\n",
    "    \n",
    "    boolq_progress_bar.update()\n",
    "\n",
    "boolq_progress_bar.close()\n",
    "\n",
    "accuracy = correct / sample_size\n",
    "\n",
    "print(f\"\\n=== BOOLQ EVALUATION RESULTS ===\")\n",
    "print(f\"Total questions: {sample_size}\")\n",
    "print(f\"Correct answers: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
