{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca023da",
   "metadata": {},
   "source": [
    "# Lab 4: LLM Fine-tuning with LoRA and Evaluation\n",
    "\n",
    "This lab covers fine-tuning language models using LoRA (Low-Rank Adaptation), data selection strategies, and comprehensive model evaluation comparing fine-tuned vs base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4c065",
   "metadata": {},
   "source": [
    "## 1. Environment and setup\n",
    "\n",
    "Authenticate with Hugging Face and verify CUDA/GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face API key from environment (do NOT hardcode your token here).\n",
    "import os\n",
    "import logging, warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Silence transformers/TRL logs early\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"trl\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hide specific noisy warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*loss_type=None.*ForCausalLMLoss.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*cuDNN SDPA backward got grad_output\\.strides\\(\\) != output\\.strides\\(\\).*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"  \n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file (if present)\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "if hf_key:\n",
    "    login(hf_key)\n",
    "else:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_API_KEY not found. Copy .env.template to .env and add your token. See Instruction.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a551ca8",
   "metadata": {},
   "source": [
    "### Verify CUDA/GPU availability\n",
    "\n",
    "Make sure we have CUDA available and a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61eaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch built with CUDA: {torch.version.cuda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e064b",
   "metadata": {},
   "source": [
    "## 2. Model initialization and quantization\n",
    "\n",
    "Initialize quantization, tokenizer, and load the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e4ee1",
   "metadata": {},
   "source": [
    "### Model initialization\n",
    "\n",
    "Initialize quantization, tokenizer, and load the base model using GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86926121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import copy\n",
    "\n",
    "# Quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Lets use GPT2\n",
    "model_name = \"gpt2\"  # This model uses safetensors format\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with safetensors (safer format)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,  # Force use of safetensors\n",
    ")\n",
    "model = copy.deepcopy(base_model)  # Create a copy of the base model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8dce67",
   "metadata": {},
   "source": [
    "## 3. Data and LoRA configuration\n",
    "\n",
    "Fetch/prepare data and configure LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d735a7",
   "metadata": {},
   "source": [
    "### Fetch dataset repository\n",
    "\n",
    "Clone the repository below for the data we will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fac8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run git clone https://github.com/ericsunkuan/ML_Spring2025_HW5.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d27c7f",
   "metadata": {},
   "source": [
    "### LoRA configuration\n",
    "\n",
    "Set LoRA rank and scaling, and apply adapters to attention modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561608f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LoRA configuration with tweaked hyperparameters\n",
    "lora_config_tweaked = LoraConfig(\n",
    "    r=16,  ### TODO : Choose any number > 0 ! Common values are 4, 8, 16, 32, 64, 128. Higher ranks allow more expressive power but also increase parameter count.\n",
    "    lora_alpha=16,  ### TODO : Choose any number > 0 ! Suggested 4, 8, 16, 32, 64, 128\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style attention modules for distilgpt2\n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    bias=\"none\",    # No bias adaptation\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config_tweaked)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "max_seq_length = 1024\n",
    "dtype = torch.float16\n",
    "load_in_4bit = True\n",
    "\n",
    "print(\"LoRA adapter applied successfully with custom hyperparameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effde0a2",
   "metadata": {},
   "source": [
    "## 4. Data processing and training setup\n",
    "\n",
    "Load dataset, create helpers, filter/sort, and prepare train/eval splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5126055b",
   "metadata": {},
   "source": [
    "### Load dataset and create helpers\n",
    "\n",
    "Load the dataset and create helper functions to format data for training. Reserve the last 50 examples for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69176",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.chat_template is None:\n",
    "    # Define a simple chat template compatible with distilgpt2\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}User: {{ message['content'] }}\\n{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\\n{% endif %}{% endfor %}Assistant:\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_from_disk(\"../../ML_Spring2025_HW5/fastchat_alpaca_52k\")\n",
    "\n",
    "# ---------------------------\n",
    "# FIRST: Reserve evaluation set from original dataset to prevent leakage\n",
    "# ---------------------------\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "# Take last 50 samples from original dataset for evaluation (before any sorting)\n",
    "eval_dataset_original = dataset.select(range(len(dataset) - 50, len(dataset)))  # Last 50 samples\n",
    "training_dataset_original = dataset.select(range(len(dataset) - 50))  # All except last 50\n",
    "\n",
    "print(f\"Reserved for evaluation (from original): {len(eval_dataset_original)}\")\n",
    "print(f\"Available for training/sorting: {len(training_dataset_original)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Add a \"text\" field to each example (for both training and evaluation datasets)\n",
    "# ---------------------------\n",
    "def add_text_field(example):\n",
    "    # Extract the first message where role == 'assistant'\n",
    "    assistant_texts = [msg[\"content\"] for msg in example[\"conversations\"] if msg[\"role\"] == \"assistant\"]\n",
    "    text = assistant_texts[0] if assistant_texts else \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Map the function over BOTH training and evaluation datasets\n",
    "training_dataset_original = training_dataset_original.map(add_text_field)\n",
    "eval_dataset_original = eval_dataset_original.map(add_text_field)\n",
    "\n",
    "# Print the dataset structure to confirm the new feature.\n",
    "print(training_dataset_original)\n",
    "print(f\"Evaluation dataset structure: {eval_dataset_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddbeda",
   "metadata": {},
   "source": [
    "### Exercise 1: Filtering and sorting\n",
    "\n",
    "Exercise 1a: Implement the data filtering function. The basic premise of fine-tuning is that we want high-quality data over a large quantity of data. Think of smart criteria to filter out low-quality data. Currently the filtering function is set to the length of the conversation, which is a bad criterion, since the model would overfit to large or small conversations. You can adjust this later, but for now, try to implement a more meaningful filtering criterion than purely the conversation length. \n",
    "\n",
    "Exercise 1b: Implement the sorting function. The sorting function is a mix of the filtering function and external metrics, for example the score in the dataset. The score is a human evaluation of the conversation, where a higher score means a better conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################### TODO : Define a meaningful helper filtering function that will be used in the sorting \n",
    "\n",
    "# The default \"conversation length\" here refers to the length of the input (human) and output (gpt), you can modify it at your will\n",
    "def compute_conversation_length(example):\n",
    "    # Compute total word count across all messages in the 'conversations' field\n",
    "    return sum(len(message[\"content\"].split()) for message in example[\"conversations\"])\n",
    "\n",
    "############## Advanced Sorting Method (TODO : Modify the sorting key) ##################\n",
    "\n",
    "def advanced_sort_key(example):\n",
    "    conversation_len = compute_conversation_length(example)\n",
    "    score = example[\"score\"]\n",
    "    return conversation_len \n",
    "\n",
    "\n",
    "\n",
    "sorted_dataset_list = sorted(training_dataset_original, key=advanced_sort_key, reverse=True)\n",
    "# Convert back to a Dataset object\n",
    "sorted_dataset = Dataset.from_list(sorted_dataset_list)\n",
    "\n",
    "print(\"\\nTop examples sorted by advanced key (combination of conversation length and score):\")\n",
    "for entry in sorted_dataset.select(range(5)):\n",
    "    print(f\"ID: {entry['id']}, Advanced Key Value: {advanced_sort_key(entry)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590114a4",
   "metadata": {},
   "source": [
    "### Exercise 2: Select training subset\n",
    "\n",
    "Get 100 samples from your sorted dataset (choose a range consistent with your sorting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c61a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# TODO ###################################################################\n",
    "train_dataset = sorted_dataset.select(range(0, 100))    ### You can experiment with different ranges\n",
    "\n",
    "# Apply formatting directly\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = eval_dataset_original.map(formatting_prompts_func, batched=True)\n",
    "print(f\"Training dataset prepared with {len(train_dataset)} examples\")\n",
    "print(f\"Evaluation dataset (reserved): {len(eval_dataset_original)} examples\")\n",
    "print(f\"Sample formatted text:\\n{train_dataset[0]['text']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0][\"conversations\"])\n",
    "print(eval_dataset[0][\"conversations\"])\n",
    "# print(train_dataset[0][\"text\"])\n",
    "print(eval_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc1635",
   "metadata": {},
   "source": [
    "### Exercise 3: Configure training hyperparameters\n",
    "\n",
    "Set the training parameters. You can experiment with different values for each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b92dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Define bfloat16 support check without unsloth\n",
    "def is_bfloat16_supported():\n",
    "    \"\"\"Check if bfloat16 is supported on current hardware\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        # Check if GPU supports bfloat16\n",
    "        return torch.cuda.get_device_capability()[0] >= 8  # Ampere and newer\n",
    "    return False\n",
    "\n",
    "################# TODO : Tweak the training hyperparameters here.  #####################\n",
    "\n",
    "training_config = {\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": # TODO \n",
    "    \"num_train_epochs\":   # TODO \n",
    "    \"learning_rate\":  # TODO \n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": # TODO \n",
    "    \"lr_scheduler_type\": \"cosine\",  \n",
    "    \"seed\": 3407,\n",
    "}\n",
    "\n",
    "################# TODO #################################################################\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    per_device_train_batch_size=training_config[\"per_device_train_batch_size\"],\n",
    "    gradient_accumulation_steps=training_config[\"gradient_accumulation_steps\"],\n",
    "    warmup_steps=training_config[\"warmup_steps\"],\n",
    "    num_train_epochs=training_config[\"num_train_epochs\"],\n",
    "    learning_rate=training_config[\"learning_rate\"],\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=training_config[\"optim\"],\n",
    "    weight_decay=training_config[\"weight_decay\"],\n",
    "    lr_scheduler_type=training_config[\"lr_scheduler_type\"],\n",
    "    seed=training_config[\"seed\"],\n",
    "    report_to=\"none\",\n",
    "    max_length=max_seq_length,  \n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    label_names = ['labels']\n",
    ")\n",
    "print(f\"Model device: {model.device}\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb693f1",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Configure trainer and run fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c3346",
   "metadata": {},
   "source": [
    "### Run training\n",
    "\n",
    "Start the training process and monitor loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d534c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99deffb8",
   "metadata": {},
   "source": [
    "## 6. Inference and evaluation\n",
    "\n",
    "Generate with the fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a2b94",
   "metadata": {},
   "source": [
    "### Fine-tuned model inference\n",
    "\n",
    "Generate responses with the fine-tuned model and compare against the base model using multiple metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05890c0c",
   "metadata": {},
   "source": [
    "### Exercise 4: Generate and collect outputs (fine-tuned model)\n",
    "\n",
    "Experiment with different values for generation parameters, such as `max_new_tokens`, `temperature`, and `top_p`. These parameters control the length, randomness, and diversity of the generated responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd192fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Set model to inference mode\n",
    "model.eval()\n",
    "\n",
    "def parse_true_output(generated_text, original_prompt):\n",
    "    \"\"\"Extract clean assistant response from generated text\"\"\"\n",
    "    # Remove the original prompt from the beginning\n",
    "    if generated_text.startswith(original_prompt):\n",
    "        response = generated_text[len(original_prompt):].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "    \n",
    "    # Clean up any extra Assistant: or User: markers\n",
    "    lines = response.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Stop at any new conversation markers\n",
    "        if line.startswith('Assistant:') or line.startswith('User:'):\n",
    "            break\n",
    "        if line:  # Only add non-empty lines\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return ' '.join(cleaned_lines).strip()\n",
    "\n",
    "# Use the reserved evaluation dataset for consistent evaluation\n",
    "print(\"Using reserved evaluation dataset for consistent evaluation...\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Initialize results storage\n",
    "inference_results = {}\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {model_device}\")\n",
    "\n",
    "# Process each entry in the evaluation set\n",
    "for index, entry in enumerate(eval_dataset):\n",
    "    entry_id = entry.get(\"id\", f\"eval_{index}\")\n",
    "\n",
    "    # Extract user message from conversations\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\" or conv.get(\"role\") == \"user\":\n",
    "            content = conv.get(\"value\", \"\") or conv.get(\"content\", \"\")\n",
    "            messages.append({\"role\": \"user\", \"content\": content})\n",
    "            break  # Only take the first human message\n",
    "\n",
    "    # Format prompt using chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Move inputs to model device\n",
    "    inputs = {key: value.to(model_device) for key, value in inputs.items()}\n",
    "\n",
    "    ################# TODO: Tweak Decoding Parameters here #####################\n",
    "    # Generate model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=50,\n",
    "            temperature=1.5,\n",
    "            top_p=0.9,\n",
    "            top_k=30,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    ################# TODO END ##########################################################\n",
    "\n",
    "    # Decode and parse outputs\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    parsed_outputs = [parse_true_output(output, formatted_prompt) for output in decoded_outputs]\n",
    "\n",
    "    # Store results\n",
    "    inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Inference completed for entry {entry_id}\")\n",
    "\n",
    "# Save results to files\n",
    "with open(\"pred.json\", \"w\") as outfile:\n",
    "    json.dump(inference_results, outfile, indent=4)\n",
    "\n",
    "with open(\"training_config.json\", \"w\") as outfile:\n",
    "    json.dump(training_config, outfile, indent=4)\n",
    "\n",
    "print(\"Inference completed for all entries in the reserved evaluation set.\")\n",
    "print(\"Results saved to pred.json and training_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbf6be",
   "metadata": {},
   "source": [
    "### Generate baseline outputs (base model)\n",
    "\n",
    "Evaluate the base (unadapted) model with the same decoding settings for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc534a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BASE MODEL INFERENCE ====================\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING BASE MODEL INFERENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make sure we're using the DistilGPT2 base_model\n",
    "base_model.eval()\n",
    "\n",
    "# Use the DistilGPT2 tokenizer (should be already set)\n",
    "print(f\"Using tokenizer: {tokenizer.name_or_path}\")\n",
    "print(f\"Base model device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "# Dictionary to store base model inference results\n",
    "base_inference_results = {}\n",
    "\n",
    "# Loop over evaluation dataset\n",
    "for index, entry in enumerate(eval_dataset):\n",
    "    entry_id = entry.get(\"id\", f\"eval_{index}\")\n",
    "\n",
    "    # Build messages (same as fine-tuned)\n",
    "    messages = []\n",
    "    for conv in entry.get(\"conversations\", []):\n",
    "        if conv.get(\"from\") == \"human\" or conv.get(\"role\") == \"user\":\n",
    "            messages.append({\"role\": \"user\", \"content\": conv.get(\"value\", \"\") or conv.get(\"content\", \"\")})\n",
    "            break\n",
    "\n",
    "    # Create inputs using chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Move to base model device\n",
    "    base_device = next(base_model.parameters()).device\n",
    "    inputs = {key: value.to(base_device) for key, value in inputs.items()}\n",
    "\n",
    "    # Generate with SAME parameters as fine-tuned model\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate( #TODO: use same settings as fine-tuned model\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=100,\n",
    "            temperature=1.5,\n",
    "            top_p=0.9,\n",
    "            top_k=30,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode outputs\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Parse outputs (same function with prompt)\n",
    "    parsed_outputs = [parse_true_output(output, formatted_prompt) for output in decoded_outputs]\n",
    "\n",
    "    # Store results\n",
    "    base_inference_results[entry_id] = {\n",
    "        \"input\": messages,\n",
    "        \"output\": parsed_outputs\n",
    "    }\n",
    "\n",
    "    print(f\"Base model inference completed for entry {entry_id}\")\n",
    "\n",
    "# Save results\n",
    "with open(\"pred_base_model.json\", \"w\") as outfile:\n",
    "    json.dump(base_inference_results, outfile, indent=4)\n",
    "\n",
    "print(\"Base model results saved to pred_base_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b43027",
   "metadata": {},
   "source": [
    "### Exercise 5: Model comparison and evaluation\n",
    "\n",
    "Run the evaluation code below to see how your fine-tuned model performs compared to the base model. Tweak all the earlier parameters, mostly focusing on the training dataset creation. This exercise is finished when you beat the base model in terms of all the evaluation metrics:\n",
    "\n",
    "- BLEU\n",
    "- ROUGE1  \n",
    "- ROUGE2\n",
    "- ROUGEL\n",
    "- Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def load_both_predictions():\n",
    "    \"\"\"Load both fine-tuned and base model predictions\"\"\"\n",
    "    # Load fine-tuned predictions\n",
    "    with open(\"pred.json\", \"r\") as f:\n",
    "        finetuned_predictions = json.load(f)\n",
    "    \n",
    "    # Load base model predictions  \n",
    "    with open(\"pred_base_model.json\", \"r\") as f:\n",
    "        base_predictions = json.load(f)\n",
    "    \n",
    "    # Use the RESERVED evaluation dataset for ground truth\n",
    "    eval_data = eval_dataset\n",
    "    \n",
    "    # Extract ground truth responses\n",
    "    references = {}\n",
    "    for index, entry in enumerate(eval_data):\n",
    "        entry_id = entry.get(\"id\", f\"eval_{index}\")\n",
    "        for conv in entry.get(\"conversations\", []):\n",
    "            if conv.get(\"from\") == \"gpt\" or conv.get(\"role\") == \"assistant\":\n",
    "                references[entry_id] = conv.get(\"value\", \"\") or conv.get(\"content\", \"\")\n",
    "                break\n",
    "    \n",
    "    return finetuned_predictions, base_predictions, references\n",
    "\n",
    "def calculate_metrics_for_model(predictions, references, model_name):\n",
    "    \"\"\"Calculate all metrics for one model\"\"\"\n",
    "    print(f\"\\n=== EVALUATING {model_name.upper()} ===\")\n",
    "    \n",
    "    # BLEU scores\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    # ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    # Coherence model\n",
    "    scoring_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    scoring_tokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    coherence_scores = []\n",
    "    \n",
    "    # Token length stats\n",
    "    pred_lengths = []\n",
    "    \n",
    "    for entry_id in predictions:\n",
    "        if entry_id in references:\n",
    "            pred_text = predictions[entry_id][\"output\"][0]\n",
    "            ref_text = references[entry_id]\n",
    "            user_message = predictions[entry_id][\"input\"][0][\"content\"]\n",
    "            \n",
    "            # Track lengths\n",
    "            pred_lengths.append(len(pred_text.split()))\n",
    "            \n",
    "            # Skip empty predictions\n",
    "            if not pred_text.strip():\n",
    "                continue\n",
    "            \n",
    "            # BLEU\n",
    "            pred_tokens = pred_text.lower().split()\n",
    "            ref_tokens = ref_text.lower().split()\n",
    "            bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # ROUGE\n",
    "            scores = scorer.score(ref_text, pred_text)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "            \n",
    "            # Coherence\n",
    "            features = scoring_tokenizer([user_message], [pred_text], \n",
    "                                       padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            scoring_model.eval()\n",
    "            with torch.no_grad():\n",
    "                score = scoring_model(**features).logits.squeeze().item()\n",
    "            coherence_scores.append(score)\n",
    "    \n",
    "    return {\n",
    "        'bleu': np.mean(bleu_scores) if bleu_scores else 0,\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0,\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0,\n",
    "        'rougeL': np.mean(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0,\n",
    "        'coherence': np.mean(coherence_scores) if coherence_scores else -10,\n",
    "        'avg_length': np.mean(pred_lengths) if pred_lengths else 0,\n",
    "        'valid_responses': len([p for p in pred_lengths if p > 0])\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load all predictions\n",
    "    finetuned_preds, base_preds, references = load_both_predictions()\n",
    "    \n",
    "    print(f\"Loaded predictions:\")\n",
    "    print(f\"  Fine-tuned: {len(finetuned_preds)} examples\")\n",
    "    print(f\"  Base model: {len(base_preds)} examples\") \n",
    "    print(f\"  References: {len(references)} examples\")\n",
    "    \n",
    "    # Calculate metrics for both models\n",
    "    finetuned_metrics = calculate_metrics_for_model(finetuned_preds, references, \"Fine-tuned\")\n",
    "    base_metrics = calculate_metrics_for_model(base_preds, references, \"Base Model\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 FINE-TUNED vs BASE MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics_names = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'coherence', 'avg_length', 'valid_responses']\n",
    "    \n",
    "    for metric in metrics_names:\n",
    "        ft_val = finetuned_metrics[metric]\n",
    "        base_val = base_metrics[metric]\n",
    "        \n",
    "        if metric in ['avg_length', 'valid_responses']:\n",
    "            improvement = ft_val - base_val\n",
    "            print(f\"{metric.upper():15}: Fine-tuned={ft_val:8.1f} | Base={base_val:8.1f} | Diff={improvement:+8.1f}\")\n",
    "        else:\n",
    "            if base_val != 0:\n",
    "                improvement_pct = ((ft_val - base_val) / abs(base_val)) * 100\n",
    "                print(f\"{metric.upper():15}: Fine-tuned={ft_val:8.4f} | Base={base_val:8.4f} | Change={improvement_pct:+6.1f}%\")\n",
    "            else:\n",
    "                print(f\"{metric.upper():15}: Fine-tuned={ft_val:8.4f} | Base={base_val:8.4f} | Change=N/A\")\n",
    "    \n",
    "    # Show sample comparisons\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📝 SAMPLE COMPARISONS (First 3 examples)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    count = 0\n",
    "    for entry_id in finetuned_preds:\n",
    "        if entry_id in base_preds and entry_id in references and count < 3:\n",
    "            count += 1\n",
    "            print(f\"\\n--- Example {count} (ID: {entry_id}) ---\")\n",
    "            print(f\"Question: {finetuned_preds[entry_id]['input'][0]['content'][:60]}...\")\n",
    "            print(f\"Fine-tuned: '{finetuned_preds[entry_id]['output'][0][:80]}...'\")\n",
    "            print(f\"Base Model:  '{base_preds[entry_id]['output'][0][:80]}...'\")\n",
    "            print(f\"Ground Truth: '{references[entry_id][:80]}...'\")\n",
    "    \n",
    "    # Summary verdict\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏆 VERDICT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    better_count = 0\n",
    "    total_metrics = 5  # bleu, rouge1, rouge2, rougeL, coherence\n",
    "    \n",
    "    for metric in ['bleu', 'rouge1', 'rouge2', 'rougeL', 'coherence']:\n",
    "        if finetuned_metrics[metric] > base_metrics[metric]:\n",
    "            better_count += 1\n",
    "    \n",
    "    if better_count >= 3:\n",
    "        print(\"✅ Fine-tuning IMPROVED the model!\")\n",
    "    elif better_count >= 2:\n",
    "        print(\"🤔 Fine-tuning shows MIXED results\")\n",
    "    else:\n",
    "        print(\"❌ Fine-tuning may have HURT performance\")\n",
    "    \n",
    "    print(f\"Fine-tuned model won {better_count}/{total_metrics} metrics\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
